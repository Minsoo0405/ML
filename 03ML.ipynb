{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 경사하강법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법이란\n",
    "    1) 정의\n",
    "        - 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘\n",
    "        - 손실 함수를 최소화하기위해 반복해서 파라미터(회귀계수)를 조정해가는 것\n",
    "    \n",
    "    2) 비유\n",
    "        - 앞이 보이지 않는 안개가 낀 산을 내려올 떄는 모든 방향으로 산을 더듬어가면서 산의 높이가 가장 낮아지는 방향으로 한발씩 내딛어 내려올 수 있다.\n",
    "\n",
    "    3) 장점\n",
    "        - 함수가 너무 복잡해 미분계수를 구하기 어려운 경우\n",
    "        - 경사하강법을 구현하는게 미분 계수를 구하는 것보다 더 쉬운 경우\n",
    "        - 데이터 양이 너무 많아 효율적인 계산이 필요한 경우\n",
    "        - '데이터를 기반으로 알고리즘이 스스로 학습한다.'는 머신러닝의 개념을 가능하게 해준 핵심 기법 중 하나\n",
    "\n",
    "    4) 정리\n",
    "        - 실제값과 예측값의 차이가 최소값이 되어야 회귀계수가 적절한 값이 되어질수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 학습률🌟\n",
    "    1) 정의\n",
    "        - learning rate\n",
    "        - 매 계산(step)마다 적용되는 이동거리\n",
    "        - 기울기 갱신의 폭 결정\n",
    "        - 하이퍼파라미터로 조절 가능함\n",
    "    \n",
    "    2) 경사하강 코드 구현\n",
    "        - 기울기가 충분히 작아지때까지 갱신하다가 기울기가 평평한 곳에 도달하면 갱신을 종료한다.\n",
    "        - 기울기 갱신횟수, 기울기 최솟값을 사전에 지정(하이퍼파라미터)\n",
    "\n",
    "    3) 경우\n",
    "        - 학습률이 너무 작은 경우\n",
    "            : 최솟값에 수렴하지 못하고 종료하게 된다.\n",
    "        - 학습률이 너무 큰 경우\n",
    "            : 최솟값에 수렴하지 못하고 발산하게 된다.\n",
    "        - 손실함수가 볼록 함수가 아니며, 비볼록 함수일 경우(대부분 비볼록함수)\n",
    "            : 최적값을 찾지 못하고, 지역 최적값(local optimum)을 구하게 될 수도 있음.\n",
    "        - 선형 회귀일 경우\n",
    "            : 선형 회귀를 위한 MSE손실(비용)함수는 볼록함수이다.\n",
    "              지역 최솟값이 없고, 하나의 전역 최솟값만 있다.\n",
    "              연속된 함수이고 기울기가 갑자기 변하지 않는다\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 경사하강법과 최소제곱법과의 차이\n",
    "    1) 경사 하강법과의 비교\n",
    "        - 경사하강법과 최소제곱법 모두 예측 알고리즘에 해당한다.\n",
    "        - 경사하강법이 수학적 최적화 알고리즘으로서 적절한 학습비율를 설정햐야하고 많은 연산량이 필요하지만 정규방정식에는 ㄱ와 같은 닩점이 없다는 장점이 있다.\n",
    "        - 정규방정식은 행렬 연산"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 배치와 에폭\n",
    "    1) 배치(batch)\n",
    "        - 1회의 경사 업데이트(스텝)에 사용되는 데이터 집합\n",
    "        - 이 떄 사용되는 데이터 집합의 개수를 배치 크기(배치 사이즈)라고 함\n",
    "        - ex) 전체 데이터 세트: 100개, 배치 사이즈: 20\n",
    "                배치 개수: 5, 경사(기울기) 업데이트 수: 5회\n",
    "            \n",
    "    2) 에폭(epoch)\n",
    "        - 전체 데이터들을 한 번 모두 사용하는 것\n",
    "        - 일반적으로 경사하강법은 수십, 수백 번 이상 에폭을 수행\n",
    "        - ex) 전체 데이터 세트: 100개, 배치 사이즈: 20, 에폭:1000\n",
    "                에폭 1회 -> 배치 개수: 5, 경사(기울기) 업데이트 수: 5회"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 경사하강법의 종류\n",
    "    1) 배치 경사하강법(batch gradient descent, full gradient descent)\n",
    "        - 매 스텝(1회의 기울기, 가중치 갱신)에서 훈련 데이터 전체를 사용(배치의 크기: 훈련 데이터 전부)하여 계산한 후 최적의 한 스텝을 나아감.\n",
    "        - 매우 큰 훈련 세트에서는 아주 느림\n",
    "        - 한 개의 배치에 전체 학습데이터가 모두 들어간다.\n",
    "\n",
    "    2) 확률적 배치 경사하강법(SGD: stochastic gradient descent)\n",
    "        - 매 스텝에서 한 개의 샘플(훈련 데이터)을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트(기울기)를 계산함\n",
    "        - 한 개의 배치와 임의의 학습 데이터 1개만 들어간ㄷ,\n",
    "        매 스텝에서 다루어야 할 뎅이터가 매우 작으르모(배치의 크기1) 처리 속도가 빠름\n",
    "        - 빠르게 최적점을 찾을 수 있지만\n",
    "        - 매우 큰 훈련 데이터도 학습 가능함.\n",
    "        - 데이터가 들어오는 즉시 가중치를 갱신할 수 있어 온라인 학습이 가능하며 즉각적인 시스템 대응이 가능\n",
    "\n",
    "    3) 미니 배치 경사하강법\n",
    "        - 확률적 경사하강법 및 경사하강법의 절충안\n",
    "        - 각 스탭에서 전체 훈련세트(배치 경사하강법)이나 하나의 샘플(확률 경사 하강법)을 기반으로 기울기를 계산하지 않고, 미니 배치라고 부르는 임의의 샘플 세트를 기반으로 기울기를 계산함.\n",
    "        - 1개의 배치에 임의의 학습 데이터 여러 개가 들어감\n",
    "        - 배치 경사하강법 보다 효율적이고 확률적 경사하강법 보다 노이즈가 적다.\n",
    "        - 행렬 연산"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 다항회귀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 다항회귀\n",
    "    1.1) 정의\n",
    "        - Polynomial Regression\n",
    "        - 독립변수가 단항식이 아닌 2차 ,3차 등으로 표현되는 것\n",
    "        - 데이터가 단순한 직선이 아닌 복잡한 형태인 경우, 산점도 상의 관측값을 통과하는 추세선을 그렸을 때 <n-1>개의 굴절이 관찰되면 이를 n차 다항식으로 모델링 함.\n",
    "        - 다중 선형회귀의 특별한 형태\n",
    "\n",
    "    1.2) 단일 속성 다항회귀 vs 다중속성 다항회귀\n",
    "        - 단일 속성 다항회귀 : 독립변수가 1개인 경우, 한개의 독립변수로 한 개의 종속변수 예측\n",
    "        - 다중속성 다항회귀: 독립변수 여러 개인 경우, 여러 개의 독립변수로 한 개의 종속변수 예측\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 사이킷런의 변환기\n",
    "\n",
    "    2.1) 정의\n",
    "        - 변환기: 특성을 만들거나 전처리하기 위한 다양한 클래스 제공\n",
    "        - 다항회귀를 위한 변환기\n",
    "            : from sklearn.processing import PolynomialFeatures\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 선형 회귀 유형 정리\n",
    "\n",
    "    |선형회귀|단일|다중|\n",
    "    |단항|||\n",
    "    |다항|||\n",
    "\n",
    "5. Underfiting VS Overfiting\n",
    "\n",
    "    - 차수가 커지게 되면 훈련데이터는 잘 맞출 수 있지만, 한번도 보지 못한 데이터에서는 아주 큰 오차가 생겨 문제가 생길 수 있다. \n",
    "    - 차수가 커지게 되면 회귀계수가 커지게 되고 훈련데이터를 제외한 새로운 데이터가 생기면 오차가 아주 크게될 수 도 있다.\n",
    "    - 잘 맞추는 모델과 오차가 가장 적은 모델을 구하는게 중요함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 최적화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "\n",
    "2. 좋은 모델이란?\n",
    "\n",
    "    2.1) 차수별 학습 VS 검증 곡선점수\n",
    "        - training score는 차수가 증가할수록 \n",
    "        - validation_score는 5차이상\n",
    "\n",
    "3. 과적합\n",
    "\n",
    "    3.1) 모델의 적합성과 일반화오차\n",
    "        - 훈련 데이터를 이용하여 학습한 모델이 아직 얻지 못한 데이터(검증 데이터)에 대한 결과값을 얼마나 정확하게 예측할 수 있는지 판단하기 위해 필요한 개념\n",
    "        - 적합도: 가지고 있는 데이터(훈련 데이터)에 대하 모델을 적용했을 때 들어맞는 정도\n",
    "\n",
    "    3.2) 과대적합(=과적합, Overfiting)\n",
    "        - 모델의 적합도는 높은데, 예측 정확도(일반화)가 낮아지는 경우\n",
    "        - 가지고 있는 훈련 데이터에 지나치게 적합한 모델을 만들면 발생\n",
    "        - 결과적으로 특정 \n",
    "\n",
    "    3.3) 과소적합(Underfiting) = 불량품\n",
    "        - 훈련 모델이 너무 단순하여 훈련 데이터의 특징을 잘 학습하지 못한 상태\n",
    "        - 데이터의 특성에 대한 설명력이 낮고 다른 데이터예측에 대한 신뢰성이 낮음.\n",
    "        - 훈련 데이터 및 검증 데이터셋이 \n",
    "\n",
    "    3.4) 과대적합의 원인과 해결방안\n",
    "        |과대적합의 원인|해결방안|\n",
    "        |훈련 데이터의 특성에 비해 학습 모델이 너무 복잡한 경우(ex. 다항식의 차수가 너무 높다|매개변수의 수가 적은 모델을 선택하거나 모델에 규제를 적용하여 단순화시킨다|\n",
    "        |훈련데이터의 특성이 특정 값으로 편증된 경우|훈련 데이터의 특성 값을 다양화한다|\n",
    "        |훈련 데이터의 개수가 너무 적은 경우|더 많은 훈련데이터를 확보한다|\n",
    "        |훈련 데이터에 노이즈가 많은 경우|오류 데이터를 수정/삭제하거나 이상치(outlier)를 제거한다|\n",
    "\n",
    "    3.5) 규제\n",
    "\n",
    "4. 과소적합\n",
    "\n",
    "    4.1) 정의\n",
    "        - 모델이 너무 단순해서 훈련데이터의 특성을 학습하지 못한 경우\n",
    "\n",
    "    4.2) 과소적합의 원인과 해겳방안\n",
    "\n",
    "|과소적합의 원인|해결방안|\n",
    "|:---:|:---:|\n",
    "|훈련 데이터의 특성에 비해 학습모델이 너무 단순한 경우|매개변수의 수가 더많은 모델을 선택하거나 새로운 특성|\n",
    "|학습모델에 지나치게 규제가 많이 적용된 경우|모델에 적용되어 있는 제약을 줄인다|\n",
    "|훈련 데이터 자체를 충분히 학습시키지 않은 경우|과대 적합이 되기 전까지 충분히 오랫동안(또는 반복하여) 학습시킨다|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
