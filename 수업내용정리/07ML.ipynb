{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 앙상블 기법\n",
    "    - 정의\n",
    "        - ensemble methods\n",
    "        - 다양한 분석 방법론을 조합해 하나의 예측 모형을 만드는 지도학습 방법론\n",
    "        - 해석이 어렵지만, 보다 우수한 예측 성능을 을 갖는다.\n",
    "    - 특징\n",
    "        - 단일 모델의 약점을 다수의 모델들을 결합하여 보완\n",
    "        - 성능이 떨어지더라도 서로 다른 유형의 모델을 섞는 것이 오히려 전체 성능에 도움이 될수 있음\n",
    "            - 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 발휘함\n",
    "            - 서로 다른 알고리즘으로 학습시키면 서로 다른 종류의 오차를 만들 가능성이 높기 때문에 정확도를 향상시킴\n",
    "        - 결정트리의 단점인 과적합을 수십~수천개의 많은 분류기를 결합해 보완하고 장점인 직관적인 분류 기준은 강화됨\n",
    "        - 정형 데이터를 다루는데 가장 뛰어난 알고리즘(비정형데이터:신경망)🌟\n",
    "    - 기법\n",
    "        - 보통(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking) 등\n",
    "        - 표\n",
    "        | |보팅|배깅|\n",
    "        |:---:|:---:|:---:|\n",
    "        |학습데이터세트|동일 데이터|서로 다른 데이터 샘플링, 중첩을 허용|\n",
    "        |알고리즘|서로 다른 알고리즘|같은 알고리즘|\n",
    "        |최종 결정 방식|보팅|보팅(동일)|\n",
    "\n",
    "    - Bootstraping 표본 추출 방법\n",
    "        - 주어진 데이터에 대해 사전 설정한 비율에 따라 복원추출(sampling with replacement, 표본을 뽑고 다시 넣음)하는 방법\n",
    "        - 100개의 관측치가 있을 때 첫번째 추출로 선택된 관측치가 두번쨰 표본 추출시에도 포함될수 있음\n",
    "        - 전체 데이터를 이용해 하나의 모델을 만드는 것보다 안정적인 예측 또는 분류 결과를 얻을 수 있음\n",
    "        - OOB(Out Of Bag) 평가\n",
    "            - Bagging으로 샘플링할때 선택되지 않은 샘플을 검증 데이터로 사용함\n",
    "            - 예측기마다 남겨진 샘플이 모두 다른 샘플\n",
    "2. 보팅(Voting) 유형\n",
    "            - 하드 보팅(직접 투표): 각 분류기가 예측한 결과를 다수결로 \n",
    "            - 소프트 보팅(간접 투포): 각 분류기가 예측한 확률을 평균하여 결정하는 방식\n",
    "            - 보팅 분류기: VotingClassifier()\n",
    "\n",
    "3. 배깅(bagging) 유형\n",
    "            - 각각의 모델에 전체 학습 데이터 중 서로다른 데이터를 샘플링하여 학습하는 점이 보팅과 차이\n",
    "            - 앙상블 모형으로 주로 결정트리 모형이 많이 쓰임\n",
    "        \n",
    "    3-1. 랜덤 포레스트(Random Forest)\n",
    "        - 결정트리를 기반으로 한 대표적인 배깅 방식\n",
    "        - 앙상블 알고리즘 중 비교적 빠른 수행속도\n",
    "        - 다양한 영역에서 높은 예측 성능을 보임\n",
    "        - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 부스팅(Boosting)\n",
    "    4-1. GBM(Gradient Boosting)\n",
    "\n",
    "    4-2. XGBoost(eXtra Gradient Boost)\n",
    "        - 트리기반 앙상블 학습에서 가장 각광받고 있는 알고리즘\n",
    "        - GBM에 기반하고 있지만, 경사하강법의 개념이 포함되어 있어 GBM의 단점인 느린 수행시간 및 과적합 규제방법의 부재 등의 문제 해결()\n",
    "        - 병렬 CPU환경에서 병렬학습이 가능하여 기존 GBM보다 빠르게 학습완료\n",
    "        \n",
    "        - 특징\n",
    "            - 표준 GBM은 과적합 규제 기능이 없으나 XGBoost는 자체에 과적합 규제 기능이 있다.\n",
    "            - Tree prunig(나무 가지치기): \n",
    "            - 자체 내장된 교차검증\n",
    "            - 결손값자체 처리 \n",
    "        - 파라미터\n",
    "            - n_estimators, learning_rate, max_depth, random_state,\n",
    "        \n",
    "        - 과적합 개선 팁\n",
    "            - learning_rate\n",
    "            - max_depth\n",
    "            - min_child\n",
    "            - min_split\n",
    "            - sub_\n",
    "\n",
    "    4-3. LightGBM\n",
    "        - 장점\n",
    "            - XGBoost보다 학습에 걸리는 시간이 짧음\n",
    "            - 더 작은 메모리 사용량 \n",
    "        - 단점\n",
    "            - 데이터 세트가 적은 경우(10000) 과적합이 발생하기 쉬움\n",
    "        - 작동방식\n",
    "            - 일반 GBM방법: 균형트리분할(Level Wise), 트리 깊이 최소화, 과적합 방지, 시간필요\n",
    "            - LightGBM: 리프 중심 트리 분할(Leaf Wise), 트리의 균형을 맞추지 않고, 트리의 깊이가 깊어지고 비대칭적인 규칙 트리를 생성, 예측오류 손실을 최소화할수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 오버샘플링 VS 언더샘플링\n",
    "    5.1. 오버샘플링(Oversampling)\n",
    "        - 정의: 많은 레이블을 가진 데이터 세트를 적은 레이블을 가진 데이터 세트 수준으로 감소\n",
    "        - SMOTE방식: K 최근접 이웃으로 데이터 신규 증식 대상 설정\n",
    "    5.2. 언더샘플링(Undersampling)\n",
    "        - 적은 레이블을 가진 데이터 세트를 많은 레이블을 가진 데이터 세트 수준으로 증식\n",
    "        - 🌟비용을 들여 데이터를 모았는데 다시 버리는 것은 비효율적\n",
    "\n",
    "6. 이상치 데이터(Outlier)\n",
    "    6.1. 정의\n",
    "        - 전체 데이터의 패턴에서 벗아난 이상 값을 가진 데이터\n",
    "    6.2. 찾는 방법\n",
    "        - IQR(Inter Quantile Range)방식\n",
    "        - 사분위(Quantile)값의 편차를 이용하는 기법\n",
    "            - 사분위: 전체 데이터를 정렬하고, 25%씩 구간으로 분할\n",
    "    6.3. 판단기준\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29022363fef606f9f23df6f4a22f2b4728bceb42fab3e5e3f1a324182233d5f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
