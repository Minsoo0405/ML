{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 머신러닝의 이해(2) - 모델 최적화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 과적합 VS 편항-분산 트레이드오프\n",
    "    - 일반화 오차 \n",
    "        - 분산 + 편향^2 + 노이즈\n",
    "    -  편향(Bias)\n",
    "        - 일반화 오차 중 잘못된 가정으로 인해 생긴 오차\n",
    "        - 데이터 또는 학습 몯델이 정받ㅂ으로부터 얼마나 특정 방향으로 치우쳐있는가를 의미함\n",
    "        - 학습 모델의 예측값이 정답과 멀리 치우친 경향이 있을 때 고편향되어 있다고 함.(과소적합)\n",
    "        - ex. 100년전 미국에서 대통령 여론조사(우편으로 받고, 가정이 잘못됨)\n",
    "    -  분산(Variance)\n",
    "        - 훈련 데이터셋에 내재된 작은 변동에 의해 발생하는 오차(정답으로부터 얼마나 많이 떨어져있는가)\n",
    "        - 데이터 또는 학습 모델이 얼마나 넓은 범위에 걸쳐 분포되어 있는가를 의미함\n",
    "        - 높은 분산값은 학습모델에서 예측값이 넓게 흩어져 있어 변동성이 높은 때를 말하며 과대적합되는 경향이 있음\n",
    "    - 편향-분산 트레이드오프\n",
    "        - 지도학습 알고리즘이 훈련 데이터셋의 범위를 넘어 지나치게 일반화하는 것을 예방하기위해 두 종류의 오차(편향, 분산)을 최소화할 때 격는 문제\n",
    "        - 이상적인 모델 복잡도는 과적합되지 않으며 특성 설명력이 충분한 것\n",
    "    - 편향과 분산의 관계\n",
    "        - 모델\n",
    "            - 저편향, 저분산: 이상적인 모델\n",
    "            - 고편향, 저분산: 과소적합(꾸준히 틀림)\n",
    "            - 고편향, 고분산: 과대적합(예측분포가 넓음)\n",
    "        - 관계\n",
    "            - 모델을 학습시킬수록 모델 복잡도는 올라감\n",
    "            - 학습을 시킬수록 편향은 줄어드나 분산은 올라감\n",
    "            - 학습이 적을수록 편향은 올라가나 분산은 내려감\n",
    "            - 과대적합: 분산이 높고 편향이 낮아짐\n",
    "            - 과소적합: 편향이 높고 분산이 낮아짐\n",
    "        - 결론\n",
    "            - 전체 오차는 편향-분산 트레이드오프 때문에 계속 학습시킨다고 해도 줄어드는 것이 아니다.\n",
    "            - 학습을 통해서 전체 오류가 최소화 되는 지점을 찾으면 그 시점에서의 추정된 모델이 가장 최적의 모델이 된다\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. K-폴드 교차검점(K-fold CV)\n",
    "\n",
    "    - K-폴드 교차검증\n",
    "        - 데이터를 무작위로 중복없이 K개의 동일한 크기의 폴드로 나눔\n",
    "        - K-1겹으로 모델을 훈련하고 나머지 하나로 성능을 평가함(각 폴드를 테스트세트로 한 번씩 사용)\n",
    "        - 즉, K번 반복하므로 K개의 서로 다른 모델을 얻을 수 있음(K폴드(겹))\n",
    "        - 각각의 폴드에서 얻은 성능을 기반으로 최종적으로 모델 성능의 평균을 계산"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 규제된 선형회귀 - 라쏘, 릿지, 엘라스틱넷 회귀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 좋은 모델이란?\n",
    "    - 현재 데이터를 잘 설명하고, 미래 데이터를 잘 예측하는 모델\n",
    "    - 복잡한 모델의 문제점: 분산이 높고(고분산, 과대적합) 노이즈가 심하다\n",
    "    - 모델 규제: 각 독립변수(특성)의 계수를 작게 만들어 과대적합을 감소시키는 것 => 단순하게 만들기\n",
    "        * 다항식의 차수를 감소시키는것\n",
    "        * 선형회귀인 경우: 모델의 가중치를 제한\n",
    "        * 릿지, 라쏘, 엘라스틱넷 회귀 모델\n",
    "\n",
    "2. 규제\n",
    "    - 정의\n",
    "        - regularization\n",
    "        - 손실함수(비용함수)에 임의의(alpha) 값으로 패널티를 부여해 회귀계수값의 크기를 감소시켜 과적합을 개선하는 방식\n",
    "\n",
    "    - 규제(정규화)목표\n",
    "        - 회귀 계수의 크기(가중치)를 제어해 과적합을 개선\n",
    "    - 규제의 효과 및 해석\n",
    "        - 과적합(고분산)된 파라미터 값에 대해 규제(패널티)를 부여하게 됨(계수를 작게하거나 높임)\n",
    "        - 규제를 추가하게 되면 규제가 없는 회귀모델보다 계수의 절대값이 작아지게 되는 원리를 이용하여 과적합 방지\n",
    "        - 규제를 가한다는 것은 푠향을 높인다는 것을 의미하며 규제가 없는 모델보다 훈련 데이터와의 적합도가 떨어지지만 전체적인 결과는 좋아짐\n",
    "        - 과적합된 모델은 지나친 노이즈를 반영할 수 있으므로 모델을 단순화하여 좀 더 일반화된 모델로 만들 필요가 있음(오컴의 면도날)\n",
    "        - 모델의 성능에 크게 기여하지 못하는 변수의 영향력을 축소하거나 필요 없는 특성 제거\n",
    "\n",
    "    -  손실함수 목표 변경🌟🌟🌟\n",
    "        - 손실함수 목표(cost) = min(MSE(w) - a*f(w))\n",
    "            * f(w): 회귀계수에 대한 함수\n",
    "            * a: 손실함수(MSE)와 f(w)의 비중을 조정하는 매개변수(하이퍼파라미터)\n",
    "        - 매개변수 a\n",
    "            * a = 0일때, 손실함수는 기존과 동일하게 적용됨다. 이때 목표는 결국 MSE값을 최소화시키는것\n",
    "            * a가 매우 큰값일때, 상대적으로 MSE값은 의미가 없고 f(w)의 값이 손실함수의 대부분을 차지하므로 전체 손실함수의 값이 커지지 않도록 w값을 0 또는 매우 작게 만들어야한다\n",
    "            * 결과적으로, a의 값을 0부터 차츰 증가시키면 회귀 계수 w값의 크기를 감소시킬수 있다.\n",
    "            * 회귀 계수 값의 크기를 감소시켜서 과대적합 문제를 개선하여 학습하는 것이 규제이다.\n",
    "\n",
    "    - 규제 전 표준화(scaling)\n",
    "        - 표준화: 특성의 스케일을 맞춤\n",
    "        - 스케일링 필요성\n",
    "            - 스케일이 한쪽 특성에 확쏠려서\n",
    "            -\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 규제의 종류\n",
    "    1-1. Ridge(L2모델)\n",
    "        \n",
    "        - 회귀모델에 L2규제(가중치의 제곱)를 추가한 것을 리지 모델이라함\n",
    "        - 규제항이 손실함에 추가됨\n",
    "        - 모델의 가중치가 가능한 작게 유지되도록 함\n",
    "        - a = 0이면 선형회귀와 동일하다\n",
    "        - a가 매우 크면 모든 가중치가 0에 가까워진다: 강한 규제\n",
    "\n",
    "    1-2. Lasso(L1모델)\n",
    "        \n",
    "        - 회귀모델에 L1규제(가중치의 절대값에 대한 규제)를 추가한 것\n",
    "        - 변수들이 무수히 많은 경우, 실질적으로 영향을 미치는 변수의 개수는 적다고 판단하고 영향이 적은 변수들의 회귀계수를 0으로 만들어 영향력 있는 변수(특성)들만 남겨놓아 특성 선택의 효과를 발생시킨다.\n",
    "        - 특성 선택에 있어 효과가 있음(종속변수와 각 독립변수간의 상관관계와 유사)\n",
    "\n",
    "    1-3. 엘라스틱넷(ElasticNet: L1+L2규제)\n",
    "        \n",
    "        - L1규제와 L2규제를 혼합한 회귀를 말함\n",
    "        - a는 규제의 강도(규제 매개변수)이고, r은 혼합비율이다\n",
    "        - 두 종류의 규제를 혼합하여 릿지 회귀와 라쏘 회귀의 절충 형태를 학습모델로 도출\n",
    "        - r = 0: L2규제와 동일하므로 릿지회귀가 됨\n",
    "        - r = 1: L1규제와 동일하므로 라쏘회귀가 됨\n",
    "        - r이 0 ~ 1사이의 값으로 릿지회귀와 라쏘회귀를 혼합"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 로지스틱 회귀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 분류\n",
    "\n",
    "        1-1) 정의    \n",
    "            - Classification\n",
    "            - 대표적인 지도학습 유형의 머신러닝 기법\n",
    "            − 데이터에 주어진 클래스 값을 학습하여 각 클래스를 구별할 수 있는 패턴을 찾아 학습 모델을 생성하고, 새로운 데이터에 대한 클래스를 예측\n",
    "        \n",
    "        1-2) 시그모이드 함수 => 0 ~ 1까의 값\n",
    "            - 로지스틱 회귀에서 사용하는 S자 함수(e:오일러 상수, 2.2718.. / z: 설명 변수의 조합)\n",
    "            - 로지스틱 회귀 ㅂㄴ응 변수의 값(y(z)): 데이터가 특정 레이블(클래스)에 속할 확률을 추정\n",
    "            - 반응변수의 값(추정된 확률) >= 임계치: 긍정(양성)클래스 예측(1)\n",
    "            - 반응변수의 값(추정된 확률) < 임계치: 부정(음성)클래스 예측(0)\n",
    "\n",
    "\n",
    "2. 로지스틱 회귀란?\n",
    "\n",
    "        1-1) 정의: 이진 분류 기능 수행 => 시그모이드 함수에 의한 S자 함수이다\n",
    "            - 🌟 로지스틱 회귀는 회귀라고 부르고 다른 회귀모형의 목적과 다르게 분류를 수행함.\n",
    "            - 첫번째 결과는 0~1까지의 실수 중 확률(성공할 확률)을 구해줌\n",
    "            - 두번째 결과는 여러가지 성공할 확률중 가장 좋은 선택인 합격 불합격을 판정해줌.\n",
    "\n",
    "        1-2) 다중 클래스 분류\n",
    "            - ex. A B C => A, Not A / B, Not B / C, Not C \n",
    "\n",
    "        1-3) 성능 평가 지표\n",
    "            1-3-1) 로지스틱 회귀의 성능 평가 지표\n",
    "                - 이진 분류 결과를 평가\n",
    "                - 오차행렬에 기반한 정확도, 정밀도, 재현율, F1 스코어, ROC_AUC 등을 사용\n",
    "            1-3-2) 오차행렬\n",
    "                - 4분면 행렬을 사용해 이진 분류의 예측 오류를 나타내는 지표\n",
    "                - 실제값과 예측값이 얼마나 매칭되었는지 표현\n",
    "            1-3-3) 용어\n",
    "                - 정확도: 예측한 결과가 실제값과 얼마나 동일한지를 나타내는 비율(전체 개수 중 정답을 맞춘 개수)\n",
    "                - 정밀도(검정력): 어떤 클래스라고 예측한 후 실제로 그 클래스를 맞춘 경우\n",
    "                - 재현율(=recall, 민감도-sensitive): 실제 positive인 데이터를 얼마나 정확히 예측했는지 평가하는 지표\n",
    "                - 제1종 오류(유의수준): 1-재현율 / 제2종 오류: 1-정밀도\n",
    "                - F1 스코어: 정밀도와 재현율을 결합한 평가지표\n",
    "                - ROC Curve(Receiver Operation Characteristics Curve): 이진 분류의 예측 성능 측정에 활용\n",
    "                - AUC(Area Under Curve): ROC 곡선 아래쪽 면적(값이 클수록 예측 성능이 좋다는 의미)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
